---
type: Notes
noteType: experience
source:
title: 心理统计基础
aliases:
memo:
createDate: 2025-09-02 10:08
modifyDate: 2025-11-17 22:38
tags:
  - needfix
  - 假设检验
  - 参数估计
cssclasses:
---

**统计包括描述统计和推断统计**，其中推断统计主要是通过部分数据来推断主体情况。

### 1. 基本概念

#### 1.1. 数据分类

不同数据类型所能应用的统计方法不同，因此需要知道数据分类，

1. 测量尺度的水平：**等比** (有绝对零点|身高体重)、**等距** (无绝对零点/均为相对值/可加减不可乘除|智商温度)、**等级/顺序** (无绝对零点|成绩排名)、**称名** (表分类不表大小|性别)
2. 变量数值的类型：离散变量、连续变量 (等比、等距)

由于不同数据的特点，所对应的统计方法不同，

- 称名量表：频率、比率；
- 等级量表：中位数、百分位数、百分比、等级相关；
- 等距量表&比率量表：平均数、标准差、积差相关、t 检验、z 检验、F 检验。

#### 1.2. 基本概念 (样本与总体)

除了要知道数据类型外，还需要知道测量的基本概念，

- 总体：需要研究的同质对象的全体，其中**参数**用于描述总体的特征，**在实际研究中是得不到的，因此需要借助统计量推断参数**；
- 个体：每一个具体研究对象；
- 样本：从总体中抽出的用以推测总体的部分对象的集合，其中**统计量**用于描述样本的特征。
- 样本容量 ($n$)：样本中包含的个体数。

| 统计指标    | 统计量       | 参数       |
| ------- | --------- | -------- |
| 平均数<br> | $\overline{X}$ | $\mu$    |
| 标准差     | $s$       | $\sigma$ |
| 相关系数    | $r$       | $\sigma$ |
| 回归系数    | $b$       | $\beta$  |

在用样本推断总体时会存在**抽样误差 (样本统计量和总体参数之间存在的由随机的偶然性导致的差异)**，导致样本统计量与总体参数不同。

#### 1.3. 经典的统计测量名词 (集中趋势&离散趋势)

**算术平均数** (arithmetic average, mean），将分布中的所有数据相加并除以数据的个数得到的值。

- $\mu=\frac{\sum{X}}{N}$ ($\mu$ 为总体平均数，$X$ 为每个个体的值), $M=\frac{\sum{X}}{n}$ ($M$ 为样本平均数，$n$ 为样本容量)

> [!info]- 加和计算规则
> $$\frac{\sum{(C+X)}}{n}=\frac{nC+\sum{X}}{n}=C+M$$
> $$\frac{\sum{(CX)}}{n}=\frac{C\sum{X}}{n}=CM$$

**加权平均数**，将两个样本合并后得到的整体组的平均数。

- $M=\frac{\sum{X_1}+\sum{X_2}}{n_1+n_2}=\frac{n_1M_1+n_2M_2}{n_1+n_2}={\color{red}\frac{n_1}{n_1+n_2}}M_1+{\color{red}\frac{n_2}{n_1+n_2}}M_2$

**中数** (median)，一组数据从大到小（或从小到大）排列，恰好位于中间位置的那个数值。

- 当 $n$ 为奇数时，$M_d / M_{dn}$ 即为中间位置的值；当 $n$ 为偶数时，$M_d / M_{dn}$ 即为中间位置的两个数的平均值。

**众数** (mode)，在一个频数分布中，众数是具有最大频率的分数或类别。

- 一个分布可能具有多个 $M_o$

---

**全距**，是在最大的 $X$ 值上实限和最小的 $X$ 值下实限之间的差值。

- $全距={URL}X_{max}-{LRL}X_{min}$

**四分位距** (四分差)，是第三个四分位数与第一个四分位数之间的距离。用中位数作集中量时，常用四分位距作变异量。

- $四分位距=Q3-Q1$ (被切三刀，第一刀处值为 $Q1$，以此类推)
- $半四分位距 = (Q3-Q1)/2$

**标准差**，以分布的的平均数作为参考点，用每个数据和平均数之间的距离来测量变异性。总体和样本的对应统计符号不同，如下为总体的符号，
- $离差=X-\mu$
- $平方和\,(平方差的和)=SS=\sum{(X-\mu)}^2=\sum{X^2}-\frac{{(\sum{X})}^2}{N}$
	- 离差平方和指的是，样本均值的平方和，$SS=\sum{(\overline{X}-\mu)^2}$
- $总体方差={\sigma}^2=\frac{SS}{N}=\frac{\sum{(X-\mu)}^2}{N}$
- $总体标准差=\sigma=\sqrt{{\sigma}^2}=\sqrt{\frac{SS}{N}}$

以下是样本的符号，

- $离差=X-M$
- $平方和=SS=\sum{(X-M)}^2$
- $样本方差=s^2=\frac{SS}{\color{red}{n-1}}=\frac{SS}{{\color{red}{df}}}=\frac{\sum{(X-M)}^2}{{\color{red}{n-1}}}，\textbf{为校正样本变异性的偏误，对样本方差和标准差的计算公式做出了调整！}$
- $样本标准差=s=\sqrt{s^2}=\sqrt{\frac{SS}{{\color{red}{df}}}}=\sqrt{\frac{SS}{{\color{red}{n-1}}}}$

> [!info]- 离差的平均数总是为 0
> $$\sum_{i=1}^{n}(X_i-\mu)=0$$

> [!info]- 每个数据与平均数差的平方和为最小
> $$\sum_{i=1}^{n}{(x_1-\overline{x})}^2\le\sum_{i=1}^{n}{(x_1-\alpha)}^2, 其中\alpha为任意值$$
> 
> 证明过程如下，
> - $\begin{align}
> \sum_{i=1}^{n}{(x_i-\alpha)}^2&=\sum_{i=1}^{n}{[(x_i-\overline{x})+(\overline{x}-\alpha)]}^2\\
> &=\sum_{i=1}^{n}{(x_i-\overline{x})}^2+\sum_{i=1}^{n}{({\color{red}\overline{x}-\alpha})}^2+2\sum_{i=1}^{n}(x_i-\overline{x})({\color{red}\overline{x}-\alpha})
> \end{align}$

这一校正引出了另一个概念，**自由度**——样本中可以自由改变的数值的个数，对于一个有 $n$ 个数值的样本，样本方差的自由度或 $df$，被定义为 $df=n-1$（由于使用到了均值，因此在已知之前的几个值后，最后一个值就固定了）。

> [!important] 为什么方差常用于推断统计？
> 推理统计目的是发现研究结果中有意义的，重要的模式。一般来说，**较小的变异性**意味着现有的模式更稳定明确，而高度变异性容易使结果发生较大的波动，从而具有不确定性。
> 
> 因此不同样本统计量之间的方差在推论统计中常用来描述随机偶然的抽样误差带来的变异，又称**抽样误差**，是推论统计中做统计推断的重要判断依据之一。

**差异系数** (coefficient of variation) 是指标准差与其算术平均数的百分比，它是没有单位的相对数，用于比较不同单位资料、相同单位但平均数差异大的资料在分布上的差异程度。
- $CV=\frac{S}{\overline{X}}\times100\%$，`5%-35%` 之间为正常值，`>35%` 表示平均数可能不能代表该分布的集中趋势，`<5%` 表示平均数和标准差的计算可能有误。

---

**标准分数** (standard scores，**Z 分数**)，原始分数减去其平均数，再除以标准差后所得到的新分数。表示该原始分数是落在平均数以上或以下几个标准差的位置上，可用于计算不同测量单位的观测值的总和或平均值。
- $Z=\frac{X-\mu}{\sigma}$ (总体上)，$Z=\frac{X-\overline{X}}{s}$ (样本上)

### 2. 概率

#### 2.1. 基本概念

> 概率是指，随机事件在试验中发生的可能性大小的数量化指标。

尽管如此，在实际操作过程中产生了对概率不同的定义，

- 先验概率：当一次试验所有可能出现的基本事件是有限个 (设为 n 个)，且每个结果出现的可能性相等，若事件 A 包含 m 个基本事件时，则：$P(A)=\frac{m}{n}$
- 经验概率：考虑到现实无法满足以上条件，只能依靠试验的结果来计算可能的概率：$P(A)=\lim_{n\to\infty}\frac{m}{n}$

> [!info]- 概率运算的基本法则
> - 概率的加法，如果 A、B 不可能同时发生，那么它们至少有一个发生的概率是：$P(A{\cup}B)=P(A)+P(B)$
> - 概率的乘法 (独立事件)，如果 A、B 相互不产生影响，那么它们同时发生的概率是：$P(A{\cap}B)=P(A)\times P(B)$

> 概率分布是指随机变量取值以及对应概率的分布，也称随机变量的分布。
> - 随机试验的结果可以用变量来表示，此时的变量是对随机现象的描述，因此称作随机变量。

在实际应用中常常根据频率分布的形态对概率分布的形态做近似的估计，根据数据的不同类型，其数据分布类型也不同，

- 离散型数据 (可枚举)：二项分布、泊松分布；
- 连续型数据 (不可枚举，需通过数学模型分析)：均匀分布、正态分布、对数正态分布、指数分布。

#### 2.2. 二项分布

> 有 $n$ 次**相互独立**的试验，任何一次试验恰好有**两个结果**，即 A 出现与 A 不出现，若事件 A 出现的概率均为 $p$，不出现的概率为 $q=1-p$，那么在 $n$ 次试验中，事件 A 出现 $x$ 次的概率为：$$P(x)=b(n,x,p)=C^x_n p^x q^{n-x}=\frac{n!}{x!(n-x)!}p^xq^{n-x}$$ 则称 $X$ 服从参数为 $n$ 和 $p$ 的**二项分布**。

如果 $np$ 和 $nq\ge10$ 时，二项分布接近正态分布，则

- $\mu=np,\,\sigma=\sqrt{npq},\,z=\frac{x-\mu}{\sigma}=\frac{x-np}{\sqrt{npq}}\,(即可通过标准正态表，查表获得相应概率)$

#### 2.3. 正态分布

> **正态分布**也称常态分布、高斯分布，是连续型随机变量中最重要的概率分布。
> 
> 若连续型随机变量 $X$ 的概率密度函数为，$$f(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$$
> 则则称 $X$ 服从正态分布，记做：$X \sim N(\mu,\sigma^2)$，其中 $x=随机变量X的取值(-\infty < x < \infty),\pi=3.14159,e=2.71828。$

其中，标准正态分布的 $\mu=0,\sigma=1$。

对于连续型随机变量，其任意区间的概率计算为 $P(a<x<b)=\int_{a}^{b}f(x)dx$

- $P(\mu - \sigma < X <\mu +\sigma)=68.3\%$
- $P(\mu - 2\sigma < X <\mu +2\sigma)=95.4\%$
- $P(\mu - 3\sigma < X <\mu +3\sigma)=99.7\%$
- $P(\mu - 1.645\sigma < X <\mu +1.645\sigma)=\color{red}90.0\%$
- $P(\mu - 1.96\sigma < X <\mu +1.96\sigma)=\color{red}95.0\%$
- $P(\mu - 2.58\sigma < X <\mu +2.58\sigma)=\color{red}99.0\%$

为了更好计算概率，可以**先将 $X$ 进行标准化**，

- $P(x_1<X<x_2)=p(\frac{x_1-\mu}{\sigma}<Z<\frac{x_2-\mu}{\sigma})$
	- 以此类推，$P(\mu-\sigma < X < \mu+\sigma)=P(-1 < Z < 1)$

#### 2.4. 抽样分布

> **抽样分布** (sampling distribution)。某个样本统计量的抽样分布，从理论上说就是在重复选取容量为 $n$ 的样本时，由每一个样本算出的该统计量数值的相对频数分布。
> - 例如，从总体的 $N$ 个元素中抽取一个容量为 $n$ 的随机样本，可以抽取出很多个容量为 $n$ 的随机样本；对于每一个样本，都可以计算出一个样本均值 $\overline{X}$，不同次抽样得到的样本均值存在差异，因此样本均值是一个随机变量；所有的样本均值形成的分布就是样本均值的抽样分布，即 $\overline{X}$ 的概率分布。

> [!important] 中心极限定理 (central limit theorem)
> 从均值为 $\mu$，方差为 $\sigma^2$ 的总体中，抽取容量为 $n$ 的随机样本，**当 $n$ 充分大时 (>30)**，样本均值 $\overline{X}$ 的抽样分布趋于服从均值为 $\mu$，方差为 $\frac{\sigma^2}{n}$ 的正态分布。
> 
> **样本平均数的标准差**叫做**标准误** $\sigma_{\overline{X}}$ (standard error)：$$\sigma_{\overline{X}}=\sqrt{\frac{\sigma^2}{n}}=\frac{\sigma}{\sqrt{n}}$$

### 3. 假设检验

#### 3.1. 基本概念

> 假设：根据已知理论与事实对研究对象所做的假定性的说明。

一般包括虚无假设和备择假设，

- **虚无假设**$H_0$：在总体中，关于没有变化、没有区别、或者没有关系的假设 (意味着满足中心极限定理)，一般对应着**实验处理无效**的假设，研究者通常希望否定它。
- **备择假设**$H_1$：在总体中，关于存在变化、有区别、或者有关联的假设。在实验中对应着**实验处理有效**的假设。是是研究者对总体的一种预期，希望得到证实。

一般来说，会通过**小概率原理**[^1] 来进行**反证法**[^2]，通过检验虚无假设是否成立进而证明备择假设。

- 如果满足 $H_0$ 的情况是小概率的，那么根据小概率原理 $H_0$ 是不可能发生的，即拒绝 $H_0$，接受 $H_1$。

但是在使用反证法时需要满足以下前提，

1. 被试样本是随机抽取的，抽取的样本具有良好代表性；
2. 样本中不同个体的观测值必须相互独立；
3. 总体标准差不发生改变。

在检验过程中，一般通过比较 **$\alpha$ 水平 (显著性水平)** 与满足 $H_0$ 假设时的概率来确定是否可以拒绝 $H_0$ 假设。

- $\alpha$ 水平：其实就是小概率事件发生的概率，如果相应事件的概率位于拒绝域 (分布的左端和右端)，则意味着是小概率事件。

#### 3.2. 假设检验过程与注意点

在了解以上概念后，我们就可以得到一个简单的假设检验过程，

1. 提出假设：
	- $H_0: \mu=\mu_0,\,H_1: \mu\ne\mu_0$ (双侧检验)
	- $H_0: \mu\ge\mu_0,\,H_1: \mu<\mu_0$ (单侧检验)
	- $H_0: \mu\le\mu_0,\,H_1: \mu>\mu_0$ (单侧检验)
2. 取得样本，计算样本统计量 (均值、$Z$ 分数)：$$Z=\frac{\overline{X}-M}{s}=\frac{\overline{X}-\mu_0}{\sigma_{\overline{X}}}=\frac{\overline{X}-\mu_0}{\frac{\sigma}{\sqrt{n}}}$$
3. 确定显著性水平 $\alpha$ (一般为 0.05、0.01、0.001)，查表得到临界；
4. 下结论，判断实际 $Z$ 分数是否落在拒绝域 (若在拒绝域内，则拒绝 $H_0$)。

---

我们知道，假设检验是通过样本来推断总体，因此会出现抽样误差，进而犯错，

1. **$\alpha$ 类错误**：$H_0$ 在正确的情况下被拒绝，该类错误发生概率与显著性水平 ($\alpha$) 相同；
2. **$\beta$ 类错误**：$H_0$ 不正确但没有被拒绝，不好计算。

![[心理统计基础.png|650]]

> [!note] 统计检验力：当处理效应存在时，正确拒绝虚无假设的能力 ($1-\beta$)。

### 4. t 检验

#### 4.1. 单样本 t 检验

根据在假设检验中求 Z 分数的公式，我们可以知道，需要得到总体的标准差 ($\sigma$) 后我们才能进行计算，$$Z=\frac{\overline{X}-\mu_0}{\sigma_{\overline{X}}}=\frac{\overline{X}-\mu_0}{\frac{\color{red}\sigma}{\sqrt{n}}}$$ 但在大多数情况下，总体标准差是未知的。总体标准差 (方差) 未知时，用样本统计量来代替总体参数，代替后原来统计量不再服从正态分布，而是服从 $t$ 分布。

当**总体标准差 $\sigma$ 未知**时，**估计标准误** $SE_{\overline{X}}$ 被用作对真实标准误 $\sigma_{\overline{X}}$ 的估计值。估计标准误是由**样本标准差**计算得到，它提供了样本均值 $\overline{X}$ 和总体均值 $\mu_0$ 之间标准距离的估计。$$\begin{align}&真实标准误\,\,\sigma_{\overline{X}}=\sqrt{\frac{\sigma^2}{n}}=\frac{\sigma}{\sqrt{n}}\\&估计标准误\,\,SE_{\overline{X}}=\sqrt{\frac{s^2}{n}}=\frac{s}{\sqrt{n}}\end{align}$$ 进而，$Z$ 分数公式中的 $\sigma_{\overline{X}}$ 也变为 $SE_{\overline{X}}$，即变成了 $t$ 分数的公式，$$\begin{align}&Z=\frac{\overline{X}-\mu_0}{\sigma_{\overline{X}}}=\frac{\overline{X}-\mu_0}{\frac{\color{red}\sigma}{\sqrt{n}}}\\&t=\frac{\overline{X}-\mu_0}{SE_{\overline{X}}}=\frac{\overline{X}-\mu_0}{\frac{\color{red}s}{\sqrt{n}}}\end{align}$$

---

> [!important] 实际研究中，我们很少进行单样本的 $t$ 检验 (样本 vs.总体)，通常会在比较两个样本时使用 $t$ 检验，
> - 独立样本研究设计/组间设计：两组数据来自两个不同的群体；
> - 重复测量研究设计/组内设计：两组数据来自同一群体或者设计上匹配群体。

#### 4.2. 独立样本 t 检验

**独立样本 $t$ 检验**通常用于比较两个样本的均值是否存在差异 ($H_1:\mu_1-\mu_2\ne0$)，其公式与单样本 $t$ 检验的公式类似，$$\begin{align}t&=\frac{样本统计量-假设的总体参数}{估计标准误}\\t&=\frac{样本均值差异-总体均值差异}{估计标准误}=\frac{(M_1-M_2)-(\mu_1-\mu_2)}{SE_{(M_1-M_2)}}\end{align}$$ 其中，$SE_{(M_1-M_2)}=\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}$（必须先满足 $\mathbf{n_1=n_2}$ 才能使用此公式）

> [!note]- $SE_{(M_1-M_2)}$ 的推导过程
> $$\begin{align}&\because 样本均值分布均为正态分布，M_1\sim N(\mu_1,\frac{\sigma_1^2}{n_1})，M_2\sim N(\mu_2,\frac{\sigma_2^2}{n_2})\\ &\therefore 根据正态分布的运算法则，M_1-M_2\sim N(\mu_1-\mu_2,\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2})\\&\therefore SE_{(M_1-M_2)}=\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}\end{align}$$

当样本量不同时，采用样本量相同时的方法得到的标准误是有偏的。**从大样本得到的方差估计比从小样本得到的方差估计更准确**，因此，在两个组的方差相等的前提下，总体方差的估计应该采用按照自由度所占比例加权的**合并方差**，

- 合并方差公式：$$S_p^2=\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{df_1+df_2}=\frac{SS_1+SS_2}{df_1+df_2}$$
- 将公式 1 中的方差替换为合并方差：$$SE_{(M_1-M_2)}=\sqrt{\frac{S_p^2}{n_1}+\frac{S_p^2}{n_2}}$$

综上，我们可以得知独立样本 $t$ 检验中的自由度 $df=df_1+df_2=(n_1-1)+(n_2-1)$

#### 4.3. 配对样本 t 检验

与独立样本 $t$ 检验的假设不同，**配对样本 $t$ 检验**的假设强调两组数据的差值，即：$H_1-H_2\ne0\rightarrow H_d\ne0$，这也导致 $t$ 的计算公式变化，$$t=\frac{样本统计量一总体参数}{估计标准误}=\frac{M_d-0}{SE_{M_d}}$$ 其中，$M_d=M_1-M_2$，$SE_{M_d}=\frac{S_d}{\sqrt{n}}$.

进行配对样本 t 检验前，需要满足以下前提假设，

1. 每种处理条件下的观测值是**相互独立**的。即在每种处理条件内 (如前测），不同被试的观测值之间相互独立；
2. 每种处理条件下的总体的**方差齐性**，即不同处理组 (前测和后测)，总体的方差满足相等的条件；
3. 对于样本量较小的情况 (如小于 30)，假设差异值 $d$ 总体上**服从正态分布**：对于样本量较大的情况，可以没有总体服从正态分布的假设。

考虑到在进行配对样本 $t$ 检验时，首先将两组数据进行一对一的运算合并，因此：$df=n-1$

#### 4.4. 效应量

考虑到假设检验的结果受到样本量的影响 (见公式)，统计上的显著性无法描述实际差异的大小或实验处理实际的意义。

- $Z$ 分数受到样本量的影响，样本量越大，越容易拒绝零假设：$Z=\frac{M-\mu_0}{\sigma_{\overline{X}}}=\frac{M-\mu_0}{\frac{\sigma}{\sqrt{\color{red}n}}}$.

因此引入不受样本量 $n$ 影响的**效应量 (effect size)** 来解释变量间的关联强度，包括 $Cohen's\,d$、$r^2$

- $Cohen's\,$：处理效应距离总体有多少个标准差，$$Cohen's\,d=\frac{均值差异}{标准差}=\frac{M-\mu}{\sigma}$$
- $r^2$：处理效应的变异百分比，即确定分数中的多少变异是由处理效应解释的，$$r^2=\frac{处理效应}{总效应}=\frac{t^2}{t^2+df}$$
---

这一公式在不同检验过程会产生不同的变式，

- 单样本 $t$ 检验 (总体标准差未知，分母使用样本标准差)：$Cohen's\,d=\frac{M-\mu_0}{s}$；
- 独立样本 $t$ 检验 ：
	- (需要使用合并方差 $S_p^2$ 的开根) $Cohen's\,d=\frac{M_1-M_2}{S_p}$;
	- (平方和 $SS$ 体现了离均差/变异 | 也可直接用 $t^2$ 计算)$$\begin{align}总的平方和(离均差平方和)，\,&SS_t=\sum{\sum{X_{ij}^2}}-\frac{{(\sum{\sum{X_{ij}^2}})}^2}{nk}\\处理（或组间）平方和，\,&SS_{处理}=SS_t-SS_{组内}=SS_t-(SS_1+SS_2)\\效应量，\,&r^2=\frac{SS_{处理}}{SS_t}=\frac{t^2}{t^2+df}\end{align}$$
- 配对样本 $t$ 检验 (由于两组样本已经合并形成了新的数据，因此就按原公式来)：
	- ($M_d$ 和 $S_d$ 均为新形成数据的均值和标准差) $Cohen's\,d=\frac{M_d}{S_d}$
	- $r^2=\frac{t^2}{t^2+df}$

### 5. 参数估计

> 参数估计，使用样本统计量估计总体参数值的推断方法。

参数估计主要有两种方法，

- 点估计：使用某样本计算得到的统计量的值来作为未知参数的估计；
- 区间估计：基于样本统计量的抽样分布，用一个区间来估计总体参数可能落入的范围，并给出落在这一区间范围内概率的大小。

其中，区间估计常被用于推论统计，以下是相关名词，

- 置信度：估计总体参数落在某一区间时的把握程度，用 $1-\alpha$ 表示，置信度往往被设定为 0.95 或 0.99；
- **置信区间**：又称置信间距，是带有置信概率的取值区间。指在某一置信度时，总体参数所在的区域范围 [^3]；通常情况下，在 $p$ 显著的情况下我们还要看置信区间是否包含 0 或者其他数，具体情况与我们的零假设相关 [^4]。
	- e.g., 如果我们需要验证 X 和 Y 之间存在回归关系，那么 $H_0: b = 0$，则置信区间最好不要过 0 点。如果过 0 点，则意味着 $b=0$ 发生的概率是可以接受的，即有可能发生。
	- e.g., OR值（logistic回归）一般看是否跨1。
- 显著性水平： 对总体参数进行区间估计时，犯错误的可能。用 $\alpha$ 表示，显著性水平往往被设定为 0.05 或 0.01。

![[心理统计基础-1.png|450]]

> [!note]- 基本原理
> 假设研究中所收集到的数据是总体的一个有**代表性**的样本，才能根据样本提供的信息，对总体进行推断；
> 
> 因此要想通过局部推断全体，首先要知道**样本统计量的抽样分布**；基于样本统计量的抽样分布，在一定的置信度下，推断总体参数可能落入的范围。

因此，我们可以得到区间估计的基本步骤，

1. 判断估计值的类型 (均值、均值差异等)：$X,\,d$
2. 计算点估计值 (样本均值、样本均值差异等): $\overline{X},\,\overline{d}$；
3. 计算标准误 (样本均值的标准误，样本均值差异的标准误)：$SE_{\overline{X}},\,SE_{M_d}$
4. 计算自由度，设定置信度，查表得到相应置信度的 t 的临界值
	1. 根据样本平均数的抽样分布，样本统计量 (以**单样本 $t$ 检验**为例)：$$t=\frac{M-\mu}{SE_M}=\frac{M-\mu}{\frac{s}{\sqrt{n}}},\,\,df=n-1$$
	2. 查表得区间：$$\begin{align}中间概率为\,1-\alpha\,的区间范围：P(-t_{\frac{\alpha}{2}}<t<t_{\frac{\alpha}{2}})&=1-\alpha\\将\,t\,带入：P(-t_{\frac{\alpha}{2}}<\frac{M-\mu}{SE_M}<t_{\frac{\alpha}{2}})&=1-\alpha\\ \color{red}P(M-SE_M\times t_{\frac{\alpha}{2}}<\mu<M+SE_M\times t_{\frac{\alpha}{2}})&\color{red}=1-\alpha\end{align}$$
	- 根据不同的 $t$ 检验类型，该式也会发生变化：$$\begin{align}独立样本\,t\,检验：&P((M_1-M_2)-SE_{(M_1-M_2)}\times t_{\frac{\alpha}{2}}<\mu<(M_1-M_2)+SE_{(M_1-M_2)}\times t_{\frac{\alpha}{2}})=1-\alpha\\   配对样本\,t\,检验：&P(M_d-SE_{M_{d}}\times t_{\frac{\alpha}{2}}<\mu<M_d+SE_{M_d}\times t_{\frac{\alpha}{2}})=1-\alpha\end{align}$$
5. 计算置信区间：$对于置信度\,95\%，查自由度为\,df\,的\,t\,分布表，得到临界值\,t_{0.025}\,(t_{\frac{\alpha}{2}})$
6. 得出结论：带入所有已知值，即可得到总体均值的 95% 的置信区间。

因此，我们就可以根据零假设的值是否在总体均值的置信区间内，来判断是否拒绝零假设。

- 如果零假设值落在置信区间内，就说明跟总体均值的差异不大，在接受范围内，则接受零假设。

> [!important] 假设检验 vs. 区间估计
> 两种推断方法用来回答不同的问题，
> 1. 假设检验常用来回答处理是否产生效应；
> 2. 参数估计常用来估计处理具有多大的效应，处理效应大致落入的范围；

### 5. 方差分析

#### 5.1. 基本概念

我们知道，$t$ 检验可以用于总体方差未知时的两两比较，但是当组数超过两组时，再进行 $t$ 检验就会使计算过程更加复杂，尤其是在探讨拥有多个水平的多个自变量对因变量的影响时，而且会导致**犯一类错误的概率变大**。

> [!info]- 为什么使用 t 检验进行两组以上的两两比较时，犯一类错误的概率变大？
> $$\begin{align}&\because\;同时满足三个\,H_0\,(\mu_1=\mu_2,\mu_2=\mu_3,\mu_1=\mu_3)\,时的概率为(0.95)^3=0.857.\\ &\therefore\;拒绝\,H_0\,的概率为(1-0.857)=0.143>0.05.\\ &\therefore\;当\,H_0\,为真，但依旧拒绝\,H_0\,的概率为0.143.\\ &\therefore一类错误概率\,\alpha=0.143>0.05\end{align}$$

因此引入 **$F$ 检验**来测量多个自变量对因变量的影响，$$F=\frac{因素水平各样本平均数之间的变异}{随机因素(误差)造成的变异}\,\,\,vs.\,\,\,t=\frac{实验组与对照组样本平均数之间的差异}{随机因素(误差)造成的差异}=\frac{\overline{X}-\mu}{SE_{\overline{X}}}$$

数据总变异包括组内变异和组间变异，其中组内变异指个体间的差异，即随机误差。

$$F=\frac{组间变异}{组内变异}=\frac{处理效应+随机误差}{随机误差}$$

根据以上定义，我们需要计算以下值，进而求得 $F$ 分数，

1. 总变异：所有被试对总体均值的差异 (平方和)；
2. 组间变异：三个组的均值到总均值的差异 (离差平方和的总和)；
3. 组内变异：各组内变量值与各组均值的离差平方和的总和。

#### 5.2. 完全随机单因素方差分析

> [!info] 方差分析的前提条件
> - 正态分布假设：样本来源的总体服从正态分布
> - 观测变量的相互独立性假设：每个处理组内部的被试相互独立
> - 方差齐性假设：各实验处理组的总体方差相同，需在方差分析前进行检验。

在满足 $F$ 检验的前提条件后，我们首先进行假设，$H_0：两两之间均不存在差异 (\mu_1=\mu_2=\mu_3)；$

其次分别计算总变异、组间变异和组内变异 (化简后)，

- 总变异：$SS_T=\sum{X^2}-\frac{(\sum{X})^2}{N}=\sum{X^2}-\frac{G^2}{N}$
	- 其中，$G$ 是总体的和。
- 组间变异：$SS_B=n_1(\overline{X}_1-\overline{X})^2+n_2(\overline{X}_2-\overline{X})^2+...+n_i(\overline{X}_i-\overline{X})^2=\sum{\frac{T^2}{n}}-\frac{G^2}{N}$
	- 其中，$n_i$ 是每个组的样本容量，$\overline{X}$ 是总体均值，$\overline{X_i}$ 是每组的样本均值；$T$ 为每组样本的和，$n$ 为每组样本的样本容量，$N$ 为总体的容量 (数据数量：$被试数\times变量的水平数$)。
- 组内变异：$SS_W=SS_T-SS_B$

之后，我们需要知道 $F$ 检验中各成分的自由度，

- 总自由度：$df_总=N-1$
- 组间自由度：$df_{组间}=k-1$ ($k$ 为组数)
- 组内自由度：$df_{组内}=\sum{(n_i-1)}=\sum{df_{每一组内}=N-k}$

然而，为了减少自由度的影响，$F$ 分数利用**均方 (MS)** 来表示 (而不是离差平方和)，$$MS=\frac{离差平方和}{自由度}\quad F=\frac{MS_{组间}}{MS_{组内}}$$

借此我们就可以得到 $F$ 分数，通过查表 ($F(df_1,df_2)=F(df_{组间},df_{组内})$)，并比较 $F$ 分数与临界值，如果超过临界值则落入拒绝域，拒绝零假设。

#### 5.3. 效应量

考虑到 $F$ 分数显著只能说明 $\mu_1\ne\mu_2\ne\mu_3$，无法确定每组间的差异程度，因此需要用效应量 $r^2\,(方差分析中为\,\eta^2)$ 来测量因变量得分之间的差异有多少可以被处理之间的差异所解释。

比较每组之间差异的方法叫做**事后检验 (Post-Hoc)**。同样的，与不采用 $t$ 检验进行三组以上时每组间差异分析的原因类似，会导致一类错误发生概率变大 (概率为 $\alpha$，与显著性水平 $\alpha$ 同值)。因此为了避免此问题，出现了以下矫正 $\alpha$ 的检验方法，

1. Tukey HSD：组间样本数相等时使用；
2. Scheffe 检验法：每一组有不相等的样本数时也可使用；
3. Bonferroni 检验法：每一组有不相等的样本数时也可使用。

#### 5.4. 单因素重复测量方差分析

通过让同一个被试进行重复测量，我们就可以通过统计得到每个个体之间的差异，因此就可以单独将个体差异导致的随机误差从组内差异中剔除 (组间差异中已经不含被试间差异了，因为比较的同一被试下的不同数据)，那么 $F$ 分数的公式就变为，$$F=\frac{处理间变异（不包含个体差异）}{误差造成的变异（不包括个体差异）}=\frac{处理效应+随机误差（不包括个体差异）}{随机误差（不包含个体差异）}$$

同样的，继续求各组分的平方和，但是为了剔除个体差异，需要多算一个被试间变异 (其他算法与完全随机单因素方差分析相同)，

- 总变异：$SS_T=\sum{X^2}-\frac{(\sum{X})^2}{N}=\sum{X^2}-\frac{G^2}{N}$
- 组间变异：$SS_B=n_1(\overline{X}_1-\overline{X})^2+n_2(\overline{X}_2-\overline{X})^2+...+n_i(\overline{X}_i-\overline{X})^2$
	- 由于每个被试的测量次数相同，因此每组的 $n$ 也相同。
- 组内变异：$SS_W=(X_{11}-\overline{X}_1)^2+(X_{21}-\overline{X}_2)^2+...+(X_{i1}-\overline{X}_i)^2$
	- $i$ 为组号，即每组中的数据与每组的均值相减。
- 被试间变异：$SS_{sbj}=k(\overline{X}_{.1}-\overline{X})^2+k(\overline{X}_{.2}-\overline{X})^2+...+k(\overline{X}_{.j}-\overline{X})^2$
	- $k$ 为组数，$j$ 为被试序号，$\overline{X}_{.j}$ 即每个被试的所有数据的均值。
- 随机误差：$SS_{error}=SS_W-SS_{sbj}$

同样的，我们也需要计算自由度，

- 总自由度：$df_总=N-1$
- 组间自由度：$df_{组间}=k-1$ ($k$ 为组数)
- 组内自由度：$df_{组内}=\sum{(n_i-1)}=\sum{df_{每一组内}=N-k}$
- 被试间自由度：$df_{sbj}=N_{sbj}-1$
- 随机误差自由度：$df_{error}=df_{组内}-df_{sbj}=(N-k)-(N_{sbj}-1)$

最后，算出组间变异和随机误差的均方，即可得到 F 分数，$$F=\frac{MS_{组间}}{MS_{error}}$$

|           | X 变量      | Y 变量     |
| --------- | -------- | ------- |
| 积差相关      | 连续、正态数据  | 连续、正态数据 |
| 斯皮尔曼      | 等级数据     | 等级数据    |
| 肯德尔       | 2 列以上等级变量 |         |
| 点二列相关     | 连续正态     | 二分名义变量  |
| $\phi$ 相关 | 二分名义变量   | 二分名义变量  |

[^1]: 小概率事件：出现概率很小的随机事件，在现实中极为罕见，几乎是不可能的；在随机抽样的条件下，一次试验抽到与总体参数相差很大
本的可能性极小，可以认为是小概率事件；在心理学研究中，一般将小于 5% 概率的随机事件称作小概率事件。
[^2]: 证明一个假设是错误的，往往比证明它是正确的要简单许多。
[^3]: [为什么置信区间跨1，假设检验就没意义 - 知乎](https://zhuanlan.zhihu.com/p/150383049)
[^4]: [什么情况下置信区间包括零是没有统计学意义，什么情况下是包括一没有统计学意义？ - 知乎](https://www.zhihu.com/question/273644403)